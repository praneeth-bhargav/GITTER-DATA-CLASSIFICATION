# Gitter Data Classification

Gitter is a modern instant messaging and collaboration platform that has revolutionized team communications and project coordination by providing a user-friendly way of organizing conversations.

The dataset leveraged for this project consists of messages from the developers in open-source communities in Gitter and some relevant characteristics of these messages.

The objective of the project is to build various models to classify the purpose of the messages in the discussions on the Gitter Platform and compare the performances of the various Natural Language Processing models developed.


## Word Embeddings
Nine Word Embedding techniques were applied:
1. CBOW
2. Skip-Gram
3. GloVe 50d
4. GloVe 100d
5. GloVe 300d
6. Word2Vec
7. fastText
8. GPT
9. GPT-2

The codes for these techniques are in Codes >> Word Embeddings

The files 1.csv - 9.csv in the Embeddings folder have been generated by applying the word embedding techniques on the input dataset in the following order:
CBOW, Skip-Gram, GloVe 50d, GloVe 100d, GloVe 300d, Word2Vec, fastText, GPT, GPT-2.

## Dimensionality Reduction

Two dimensionality reduction techniques were used:

1. One-Way ANOVA Test
- This test is applied to discard the non-influential features from the data.
- One-Way ANOVA Test is applied on files 1.csv - 9.csv to generate the files 10.csv - 18.csv respectively.

2. Principal Component Analysis (PCA)
- This technique is used to generate a smaller number of uncorrelated features from the given data.
- One-Way ANOVA Test is applied on files 1.csv - 9.csv to generate the files 19.csv - 27.csv respectively.

The codes for these techniques are in Codes >> Feature Selection Techniques

## Data Sampling

The Synthetic Minority Oversampling Technique (SMOTE) technique was used to sample data to solve the class imbalance problem.

The code for SMOTE is in Codes >> SMOTE

Files 28.csv - 54.csv in the Embeddings folder were generated corresponding to the first 27 files using this technique.

## Classification Algorithms

The following algorithms were applied to classify the purpose of the messages

- Naive Bayes (Multinomial, Bernoulli and Gaussian)
- Decision Tree 
- Logistic Regression
- K Nearest Neighbors (KNN)
- KNN, Multinomial NB, Logistic Regression, and Decision Tree with Bagging
- Random Forest
- Extra Trees
- Ada Boost
- Gradient Boosting
- Multi-Layer Perceptron with Limited-memory BFGS, Stochastic Gradient Descent and Adam Optimizers 

Both One-vs-Rest and One-vs-One for Multi-Class Classification strategies were applied for all the algorithms mentioned above.

The code for classification algorithms is in Codes >> Classification

## Prediction Files

The Embeddings folder consists of the files 1.csv - 54.csv, which are the files generated after applying the word embedding, dimensionality reduction, and data sampling techniques on the dataset.

The Outputs folder consists of files 1pred.csv - 54pred.csv and 1predp.csv - 54predp.csv, which correspond to the prediction of the purpose made by the classification algorithms on the 54 input files.

Each xpred.csv file consists of 35 columns:
- The first 34 columns correspond to the predictions of 34 classification algorithms (17 one vs rest and 17 one vs one algorithm) on the x.csv file data.
- The last column corresponds to the ground truth labels.

Each xpredp.csv file consists of 52 columns:
- Each set of 3 columns consists of the class probabilities for each data point for the 17 one vs rest classification algorithms applied.
- The last column corresponds to the ground truth labels.

## Results

The Results Folder consists of the Box Plot Results of the Accuracy, Sensitivity, Specificity and Geometric Means of:
- Word-embedding techniques
- Feature Selection Techniques and the Original Feature Set
- Original Data vs SMOTE Synthesized Data
- Classification Algorithms
- One vs One and One vs Rest Strategies
